{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, datasets, pandas\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from safetensors.torch import load_file\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from utils.config import ID_TO_LABEL, LABEL_TO_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "    labels = [torch.tensor(item[\"labels\"], dtype=torch.float32) for item in batch]\n",
    "\n",
    "    # Pad labels to the maximum label length in the dataset\n",
    "    labels_padded = pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return {\"text\": texts, \"labels\": labels_padded}\n",
    "\n",
    "def to_one_hot(indices, num_classes):\n",
    "    # if num_classes == 2:\n",
    "    #     return indices.long().numpy()\n",
    "    one_hot = torch.zeros((indices.shape[0], num_classes))\n",
    "    \n",
    "    one_hot.scatter_(1, indices.long(), 1)\n",
    "    return one_hot.long().numpy()\n",
    "\n",
    "def binarize(preds):\n",
    "    # 1 is unfair, 0 is fair. Assuming the first element corresponds to the \"fair\" category\n",
    "    return [1 if el[0] != 1 else 0 for el in preds]\n",
    "    \n",
    "def run_metrics(preds, labels, to_binarize, label_to_id):\n",
    "    if to_binarize:\n",
    "        labels = binarize(labels)\n",
    "        preds = binarize(preds)\n",
    "\n",
    "    micro_f1 = f1_score(labels, preds, average=\"micro\", zero_division=0)\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Micro F1 Score: {micro_f1:.4f}\")\n",
    "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "    report = classification_report(labels, preds, zero_division=0, output_dict=True)\n",
    "    # report = classification_report(labels, preds, zero_division=0, target_names=label_to_id, output_dict=True)\n",
    "    df = pandas.DataFrame(report).transpose()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, seed, num_labels, is_binary, use_heads):\n",
    "    if use_heads and not is_binary:\n",
    "        print(\"Use a binary model for the multi-head approach\")\n",
    "        return \n",
    "    if is_binary:\n",
    "        seed = str(seed)+\"_b\"\n",
    "    safetensors_path = f\"logs/unfair_tos/{model_name}/seed_{seed}/model.safetensors\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize the model and tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    model.load_state_dict(load_file(safetensors_path))\n",
    "    model.to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    if use_heads:\n",
    "        head_model_name = \"prajjwal1/bert-mini\"\n",
    "        head_models = dict.fromkeys(LABEL_TO_ID.keys())\n",
    "        for cat in head_models.keys():\n",
    "            if cat != \"fair\":\n",
    "                head = AutoModelForSequenceClassification.from_pretrained(head_model_name, num_labels=2)\n",
    "                safetensors_path = f\"logs/unfair_tos_head/{head_model_name}/{cat}/seed_1/model.safetensors\"\n",
    "                head.load_state_dict(load_file(safetensors_path))\n",
    "                head.to(device)\n",
    "                head_models[cat] = head\n",
    "\n",
    "\n",
    "    # Load the test dataset\n",
    "    full_dataset = datasets.load_from_disk(\"./142_dataset/tos.hf/\")\n",
    "\n",
    "    test_dataset = full_dataset[\"test\"]\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = tokenizer(batch['text'], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            # outputs = model(**batch[\"inputs\"].to(device))\n",
    "            outputs = model(**inputs)\n",
    "            all_logits.extend(outputs.logits)\n",
    "            preds = (torch.sigmoid(outputs.logits) > 0.5).int()\n",
    "\n",
    "            if is_binary and not use_heads:\n",
    "                # aggregate unfair categories (e.g. [2, 5] --> [0, 1])\n",
    "                batch[\"labels\"] = (batch[\"labels\"].sum(dim=1) > 0).float().unsqueeze(1)\n",
    "            elif is_binary and use_heads:\n",
    "                print(preds)\n",
    "                pass\n",
    "\n",
    "            all_preds.extend(preds.cpu().long().numpy())\n",
    "            all_labels.extend(to_one_hot(batch[\"labels\"], num_labels))\n",
    "\n",
    "    return all_preds, all_labels, all_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlpaueb/legal-bert-base-uncased\"  # (2, 2)\n",
    "# model_name = \"microsoft/deberta-base\" # (5, 4)\n",
    "# model_name = \"zlucia/custom-legalbert\"  # (1, 2)\n",
    "# model_name = \"roberta-large\"\n",
    "# model_name = \"allenai/longformer-base-4096\"\n",
    "seed_multi = 2\n",
    "seed_bin = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BertForSequenceClassification:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([10, 768]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([10]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Multi-head model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m preds, labels, logits \u001b[38;5;241m=\u001b[39m evaluate_model(model_name, seed_bin, \u001b[38;5;28mlen\u001b[39m(LABEL_TO_ID), is_binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, use_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model_name, seed, num_labels, is_binary, use_heads)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize the model and tokenizer\u001b[39;00m\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, num_labels\u001b[38;5;241m=\u001b[39mnum_labels)\n\u001b[0;32m---> 13\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(load_file(safetensors_path))\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BertForSequenceClassification:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([10, 768]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([10])."
     ]
    }
   ],
   "source": [
    "# Multi-head model\n",
    "preds, labels, logits = evaluate_model(model_name, seed_bin, len(LABEL_TO_ID), is_binary=True, use_heads=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary model\n",
    "preds, labels, logits = evaluate_model(model_name, seed_bin, 2, is_binary=True, use_heads=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 Score: 0.9617\n",
      "Macro F1 Score: 0.8792\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.981850</td>\n",
       "      <td>0.979012</td>\n",
       "      <td>3967.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.803815</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.779392</td>\n",
       "      <td>390.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro avg</th>\n",
       "      <td>0.961671</td>\n",
       "      <td>0.961671</td>\n",
       "      <td>0.961671</td>\n",
       "      <td>4357.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.890003</td>\n",
       "      <td>0.869130</td>\n",
       "      <td>0.879202</td>\n",
       "      <td>4357.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.960761</td>\n",
       "      <td>0.961671</td>\n",
       "      <td>0.961144</td>\n",
       "      <td>4357.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>samples avg</th>\n",
       "      <td>0.961671</td>\n",
       "      <td>0.961671</td>\n",
       "      <td>0.961671</td>\n",
       "      <td>4357.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score  support\n",
       "0              0.976190  0.981850  0.979012   3967.0\n",
       "1              0.803815  0.756410  0.779392    390.0\n",
       "micro avg      0.961671  0.961671  0.961671   4357.0\n",
       "macro avg      0.890003  0.869130  0.879202   4357.0\n",
       "weighted avg   0.960761  0.961671  0.961144   4357.0\n",
       "samples avg    0.961671  0.961671  0.961671   4357.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_metrics(preds, labels, False, {\"fair\":0, \"unfair\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Multi label model\n",
    "preds, labels = evaluate_model(model_name, seed_multi, len(LABEL_TO_ID), is_binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 Score: 0.9617\n",
      "Macro F1 Score: 0.8762\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fair</th>\n",
       "      <td>0.977227</td>\n",
       "      <td>0.980909</td>\n",
       "      <td>0.979065</td>\n",
       "      <td>3981.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unfair</th>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.757979</td>\n",
       "      <td>0.773406</td>\n",
       "      <td>376.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.961671</td>\n",
       "      <td>0.961671</td>\n",
       "      <td>0.961671</td>\n",
       "      <td>0.961671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.883350</td>\n",
       "      <td>0.869444</td>\n",
       "      <td>0.876235</td>\n",
       "      <td>4357.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.961024</td>\n",
       "      <td>0.961671</td>\n",
       "      <td>0.961317</td>\n",
       "      <td>4357.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "fair           0.977227  0.980909  0.979065  3981.000000\n",
       "unfair         0.789474  0.757979  0.773406   376.000000\n",
       "accuracy       0.961671  0.961671  0.961671     0.961671\n",
       "macro avg      0.883350  0.869444  0.876235  4357.000000\n",
       "weighted avg   0.961024  0.961671  0.961317  4357.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binarize label at test time\n",
    "df_bin = run_metrics(preds, labels, True, {\"fair\":0, \"unfair\":1})\n",
    "df_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 Score: 0.9575\n",
      "Macro F1 Score: 0.7779\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fair</th>\n",
       "      <td>0.977227</td>\n",
       "      <td>0.980909</td>\n",
       "      <td>0.979065</td>\n",
       "      <td>3981.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ch</th>\n",
       "      <td>0.732394</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cr</th>\n",
       "      <td>0.677419</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j</th>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>law</th>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ltd</th>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.664336</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ter</th>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.817204</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pinc</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro avg</th>\n",
       "      <td>0.958901</td>\n",
       "      <td>0.956079</td>\n",
       "      <td>0.957488</td>\n",
       "      <td>4417.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.825778</td>\n",
       "      <td>0.746600</td>\n",
       "      <td>0.777934</td>\n",
       "      <td>4417.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.958494</td>\n",
       "      <td>0.956079</td>\n",
       "      <td>0.956736</td>\n",
       "      <td>4417.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>samples avg</th>\n",
       "      <td>0.962359</td>\n",
       "      <td>0.961518</td>\n",
       "      <td>0.960676</td>\n",
       "      <td>4417.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score  support\n",
       "fair           0.977227  0.980909  0.979065   3981.0\n",
       "a              0.818182  0.562500  0.666667     16.0\n",
       "ch             0.732394  0.800000  0.764706     65.0\n",
       "cr             0.677419  0.724138  0.700000     29.0\n",
       "j              0.956522  0.846154  0.897959     26.0\n",
       "law            0.954545  0.913043  0.933333     23.0\n",
       "ltd            0.772358  0.664336  0.714286    143.0\n",
       "ter            0.692308  0.782609  0.734694     69.0\n",
       "use            0.926829  0.730769  0.817204     52.0\n",
       "pinc           0.750000  0.461538  0.571429     13.0\n",
       "micro avg      0.958901  0.956079  0.957488   4417.0\n",
       "macro avg      0.825778  0.746600  0.777934   4417.0\n",
       "weighted avg   0.958494  0.956079  0.956736   4417.0\n",
       "samples avg    0.962359  0.961518  0.960676   4417.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classes = run_metrics(preds, labels, False, LABEL_TO_ID)\n",
    "df_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write false positive and false negatives to files\n",
    "full_dataset = datasets.load_from_disk(\"./142_dataset/tos.hf/\")\n",
    "test_dataset = full_dataset[\"test\"]\n",
    "\n",
    "false_positive_texts = []\n",
    "false_pos_classes = {v: [] for k,v in ID_TO_LABEL.items()}\n",
    "false_neg_classes = {v: [] for k,v in ID_TO_LABEL.items()}\n",
    "for i, (label, pred, data) in enumerate(zip(labels, preds, test_dataset)):\n",
    "    for j in range(len(label)):\n",
    "        # False positive\n",
    "        if pred[j] == 1 and label[j] == 0: \n",
    "            true_labels = [ID_TO_LABEL[el] for el in np.where(label == 1)[0]]\n",
    "            false_pos_classes[ID_TO_LABEL[j]].append((data['text'], true_labels))\n",
    "        \n",
    "        # False negative\n",
    "        if pred[j] == 0 and label[j] == 1:\n",
    "            false_labels = [ID_TO_LABEL[el] for el in np.where(pred == 1)[0]]\n",
    "            false_neg_classes[ID_TO_LABEL[j]].append((data['text'], false_labels))   \n",
    "\n",
    "for cls in false_pos_classes.keys():\n",
    "    with open(f\"./out/fp_{cls}.txt\", \"w\") as f:\n",
    "        for el in false_pos_classes[cls]:\n",
    "            f.write(f\"clause: {el[0]}\\t true: {el[1]}\\n\")\n",
    "            # print(el)\n",
    "    # print(false_pos_classes[cls])\n",
    "\n",
    "\n",
    "for cls in false_neg_classes.keys():\n",
    "    with open(f\"./out/fn_{cls}.txt\", \"w\") as f:\n",
    "        for el in false_neg_classes[cls]:\n",
    "            f.write(f\"clause: {el[0]}\\t prediction: {el[1]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
